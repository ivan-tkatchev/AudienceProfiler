{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "anonymous-complement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-specification",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-underwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "decent-emerald",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna(0)\n",
    "data = data.replace('None', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "annual-negotiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.pickle', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sharp-egypt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - 2%\n",
    "# 2 - 3\n",
    "# 3 - 31\n",
    "# 4 - 24\n",
    "# 5 - 38\n",
    "\n",
    "\n",
    "num_test = {1: 0.02, 2:0.03, 3:0.31, 4: 0.24, 5:0.38}\n",
    "test = []\n",
    "for segment, num in num_test.items():\n",
    "    test.append(data.loc[data['Segment'] == segment].sample(n=int(num*11213629)))\n",
    "test = pd.concat(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-teach",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = list(pd.read_csv('my_test.csv')['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-focus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['gamecategory'].fillna('Games',inplace=True)\n",
    "# data['subgamecategory'].replace('Puzzle', name,inplace=True)\n",
    "\n",
    "# data['oblast'].fillna('Москва',inplace=True)\n",
    "# data['city'].replace('Москва', name,inplace=True)\n",
    "\n",
    "# data['os'].fillna('android',inplace=True)\n",
    "# data['osv'].replace('10.0', name,inplace=True)\n",
    "# data['osv_numerical'].replace('0.890756', name,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-acoustic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[['bundle', 'bundle_hash']].value_counts(sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-wrapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna = {}\n",
    "# for i in t.columns:\n",
    "#     a = t[i].value_counts(sort=True)\n",
    "#     b = pd.DataFrame(a)\n",
    "#     name = b.iloc[0].name\n",
    "#     fillna[i] = name\n",
    "#     t[i].fillna(name,inplace=True)\n",
    "#     t[i].replace('None', name,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-munich",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna = {}\n",
    "# for i in data.columns:\n",
    "#     a = data[i].value_counts(sort=True)\n",
    "#     b = pd.DataFrame(a)\n",
    "#     name = b.iloc[0].name\n",
    "#     fillna[i] = name\n",
    "#     data[i].fillna(name,inplace=True)\n",
    "#     data[i].replace('None', name,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-cycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_test_cats = {}\n",
    "# for i in set(t['gamecategory']):\n",
    "#     cur_cat = t.loc[t['gamecategory'] == i]\n",
    "#     dict_test_cats[i] = {}\n",
    "#     for j in set(cur_cat['subgamecategory']):\n",
    "#         num = cur_cat.loc[cur_cat['subgamecategory'] == j].shape[0]\n",
    "#         dict_test_cats[i][j] = num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-living",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-alarm",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-representative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_subs = []\n",
    "# for i in dict_test_cats.keys():\n",
    "#     sub_data = data.loc[data['gamecategory'] == i]\n",
    "#     for j in dict_test_cats[i].keys():\n",
    "#         num = dict_test_cats[i][j]\n",
    "#         if  num > 0:\n",
    "#             sub_sub_data = sub_data.loc[sub_data['subgamecategory'] == j]\n",
    "#             if sub_sub_data.shape[0] <= num:\n",
    "#                 print(num, sub_sub_data.shape[0], i, j)\n",
    "#                 num =sub_sub_data.shape[0] \n",
    "#             sub_test = sub_sub_data.sample(n = num)\n",
    "#             test_subs.append(sub_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "blocked-royalty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33865163, 20) (10989353, 20)\n"
     ]
    }
   ],
   "source": [
    "test_ids = list(test['id'])\n",
    "train = data.loc[~data['id'].isin(test_ids)]\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "black-blink",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-satisfaction",
   "metadata": {},
   "source": [
    "<h1> Selecting & encoding features </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-ancient",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_features(data):\n",
    "    data['interp_game'] = data['interp_game'].astype(str)\n",
    "    data['interp_subgame'] = data['interp_subgame'].astype(str)\n",
    "    data['city'] = data['interp_subgame'].astype(str)\n",
    "    data['oblast'] = data['oblast'].astype(str)\n",
    "    data['os'] = data['os'].astype(str)\n",
    "    data['shift'] = data['shift'].astype(str)\n",
    "\n",
    "    data_dict = {}\n",
    "    features_to_encoding = ['interp_game','oblast','interp_subgame','city','os','shift','day_of_week']\n",
    "    using_features = ['Segment', 'shift', 'oblast', 'city', 'os', 'interp_game',\n",
    "           'interp_subgame', 'osv_numerical',  'day_of_week',\n",
    "           'is_weekend', 'hour', 'dist_from_msk_in_tz_hours','id']\n",
    "\n",
    "    for feature in features_to_encoding:\n",
    "        le = LabelEncoder()\n",
    "        print(feature)\n",
    "        data_dict[feature] = np.array(le.fit_transform(np.array(data[feature])))\n",
    "\n",
    "\n",
    "    for category in set(using_features) - set(features_to_encoding):\n",
    "        data_dict[category] = np.array(data[category])\n",
    "    return data_dict\n",
    "data_dict = encode_features(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-american",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_test = np.where(np.isin(data_dict['id'], test_ids))[0]\n",
    "indexes_train = list(set(list(range(0, data_dict['id'].shape[0]))) - set(indexes_test))\n",
    "train_data_dict = {}\n",
    "test_data_dict = {}\n",
    "for category in set(using_features):\n",
    "    train_data_dict[category] = np.array(data_dict[category][indexes_train])\n",
    "    train_data_dict[category] =  train_data_dict[category][::2]\n",
    "    test_data_dict[category] = np.array(data_dict[category][indexes_test])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-retention",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-incident",
   "metadata": {},
   "source": [
    "<h1> Balance dataset </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-functionality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment 1 balanced. Was 697359 size. Now: 11855103\n",
      "Segment 2 balanced. Was 1079837 size. Now: 10798370\n",
      "Segment 3 balanced. Was 10710830 size. Now: 12926327\n",
      "Segment 4 balanced. Was 8450810 size. Now: 12926327\n"
     ]
    }
   ],
   "source": [
    "segments = list(range(1, 6))\n",
    "size_segments = {}\n",
    "for i in segments:\n",
    "    size_segments[i] = np.where(train_data_dict['Segment'] == i)[0].shape[0]\n",
    "\n",
    "max_size = np.max(list(size_segments.values()))\n",
    "\n",
    "all_balanced_data = {}\n",
    "for segment in segments:\n",
    "    diff = size_segments[segment] - max_size\n",
    "    \n",
    "    if diff < 0 and abs(diff) > size_segments[segment]:\n",
    "        cur_data_segment = {}\n",
    "        indexes = np.where(train_data_dict['Segment'] == segment)[0] \n",
    "        for feature in using_features:\n",
    "            cur_data_segment[feature] = train_data_dict[feature][indexes]\n",
    "        final_data = {}\n",
    "        count_repeated = abs(diff) // size_segments[segment]\n",
    "        if count_repeated == 1:\n",
    "            count_repeated += 1\n",
    "        for feature in using_features:\n",
    "            if feature not in all_balanced_data.keys():\n",
    "                all_balanced_data[feature] = np.tile(cur_data_segment[feature], count_repeated)\n",
    "            else:\n",
    "                all_balanced_data[feature] = np.concatenate((all_balanced_data[feature], np.tile(cur_data_segment[feature], count_repeated)))\n",
    "    elif diff < 0:\n",
    "        cur_data_segment = {}\n",
    "        indexes = np.where(train_data_dict['Segment'] == segment)[0] \n",
    "        for feature in using_features:\n",
    "            cur_data_segment[feature] = train_data_dict[feature][indexes]\n",
    "            \n",
    "        indexes = np.array(random.sample(list(range(0, cur_data_segment['Segment'].shape[0])), abs(diff))).astype(int)\n",
    "        for feature in using_features:\n",
    "            if feature not in all_balanced_data.keys():\n",
    "                all_balanced_data[feature] = np.concatenate((cur_data_segment[feature],cur_data_segment[feature][indexes]))\n",
    "            else:\n",
    "                tmp = np.concatenate((cur_data_segment[feature],cur_data_segment[feature][indexes]))\n",
    "                all_balanced_data[feature] = np.concatenate((all_balanced_data[feature], tmp))\n",
    "    else:\n",
    "        cur_data_segment = {}\n",
    "        indexes = np.where(train_data_dict['Segment'] == segment)[0] \n",
    "        for feature in using_features:\n",
    "            cur_data_segment[feature] = train_data_dict[feature][indexes]\n",
    "        for feature in using_features:\n",
    "            if feature not in all_balanced_data.keys():\n",
    "                all_balanced_data[feature] = cur_data_segment[feature]\n",
    "            else:\n",
    "                all_balanced_data[feature] = np.concatenate((all_balanced_data[feature], cur_data_segment[feature]))\n",
    "    now_size = np.where(all_balanced_data['Segment'] == segment)[0].shape[0]\n",
    "    print(f'Segment {segment} balanced. Was {size_segments[segment]} size. Now: {now_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-houston",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in using_features:\n",
    "    all_balanced_data[i] = all_balanced_data[i][::5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hindu-grass",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(data_dict_cur):\n",
    "    data_dict_cur['hour'] = data_dict_cur['hour'].astype(int)\n",
    "    data_dict_cur['hour'] = (data_dict_cur['hour'] - np.min(data_dict_cur['hour'])) / np.max(data_dict_cur['hour']) - np.min(data_dict_cur['hour'])\n",
    "    data_dict_cur['dist_from_msk_in_tz_hours'] = (data_dict_cur['dist_from_msk_in_tz_hours'] - \\\n",
    "                                                      np.min(data_dict_cur['dist_from_msk_in_tz_hours'])) / \\\n",
    "    np.max(data_dict_cur['dist_from_msk_in_tz_hours']) - np.min(data_dict_cur['dist_from_msk_in_tz_hours'])\n",
    "    return data_dict_cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "reasonable-organization",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dict = fun(train_data_dict)\n",
    "test_data_dict = fun(test_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "former-familiar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'os': array([1, 2, 1, ..., 2, 2, 2]),\n",
       " 'day_of_week': array([2, 5, 3, ..., 3, 6, 0]),\n",
       " 'is_weekend': array([0, 1, 0, ..., 0, 1, 0]),\n",
       " 'dist_from_msk_in_tz_hours': array([0.        , 0.22222222, 0.        , ..., 0.33333333, 0.        ,\n",
       "        0.        ]),\n",
       " 'interp_game': array([0, 8, 1, ..., 8, 8, 8]),\n",
       " 'city': array([ 0, 13, 34, ..., 37, 37, 37]),\n",
       " 'shift': array([1, 3, 1, ..., 4, 1, 1]),\n",
       " 'osv_numerical': array([0.8907563 , 0.92156863, 0.96638655, ..., 0.90849673, 0.87581699,\n",
       "        0.97385621]),\n",
       " 'hour': array([0.60869565, 0.65217391, 0.13043478, ..., 0.95652174, 0.30434783,\n",
       "        0.95652174]),\n",
       " 'interp_subgame': array([ 0, 13, 34, ..., 37, 37, 37]),\n",
       " 'oblast': array([48, 69, 90, ..., 54, 48, 65]),\n",
       " 'id': array([       0,        2,        4, ..., 44854512, 44854514, 44854515]),\n",
       " 'Segment': array([5, 5, 5, ..., 3, 3, 5])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-equality",
   "metadata": {},
   "source": [
    "<h1> Train-validation-test dataset </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "imperial-highlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_x_y(cur_data):\n",
    "    x = np.empty((cur_data['Segment'].shape[0],len(using_features)-1))\n",
    "    for i, feature in zip(range(0, 12),['shift', 'oblast', 'city', 'os', 'interp_game',\n",
    "       'interp_subgame', 'osv_numerical',  'day_of_week',\n",
    "       'is_weekend', 'hour', 'dist_from_msk_in_tz_hours','id']):\n",
    "        x[:,i] = cur_data[feature]\n",
    "\n",
    "    y = np.array(pd.get_dummies(cur_data['Segment']))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-ceremony",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "hourly-straight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shift\n",
      "oblast\n",
      "city\n",
      "os\n",
      "interp_game\n",
      "interp_subgame\n",
      "osv_numerical\n",
      "day_of_week\n",
      "is_weekend\n",
      "hour\n",
      "dist_from_msk_in_tz_hours\n",
      "id\n",
      "shift\n",
      "oblast\n",
      "city\n",
      "os\n",
      "interp_game\n",
      "interp_subgame\n",
      "osv_numerical\n",
      "day_of_week\n",
      "is_weekend\n",
      "hour\n",
      "dist_from_msk_in_tz_hours\n",
      "id\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = get_train_x_y(train_data_dict)\n",
    "x_test, y_test = get_train_x_y(test_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "prompt-entity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.33, random_state=44)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-subject",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-western",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "suited-distributor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5006353\ttest: 0.5006297\tbest: 0.5006297 (0)\ttotal: 4.12s\tremaining: 37.1s\n",
      "1:\tlearn: 0.4309862\ttest: 0.4309580\tbest: 0.4309580 (1)\ttotal: 8.3s\tremaining: 33.2s\n",
      "2:\tlearn: 0.4005583\ttest: 0.4005148\tbest: 0.4005148 (2)\ttotal: 12.4s\tremaining: 28.9s\n",
      "3:\tlearn: 0.3830548\ttest: 0.3829906\tbest: 0.3829906 (3)\ttotal: 16.4s\tremaining: 24.6s\n",
      "4:\tlearn: 0.3744401\ttest: 0.3743616\tbest: 0.3743616 (4)\ttotal: 20.7s\tremaining: 20.7s\n",
      "5:\tlearn: 0.3699885\ttest: 0.3698727\tbest: 0.3698727 (5)\ttotal: 24.6s\tremaining: 16.4s\n",
      "6:\tlearn: 0.3670408\ttest: 0.3669159\tbest: 0.3669159 (6)\ttotal: 28.7s\tremaining: 12.3s\n",
      "7:\tlearn: 0.3647797\ttest: 0.3646420\tbest: 0.3646420 (7)\ttotal: 32.8s\tremaining: 8.2s\n",
      "8:\tlearn: 0.3639202\ttest: 0.3637765\tbest: 0.3637765 (8)\ttotal: 36.9s\tremaining: 4.11s\n",
      "9:\tlearn: 0.3630247\ttest: 0.3628694\tbest: 0.3628694 (9)\ttotal: 41s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.3628693628\n",
      "bestIteration = 9\n",
      "\n",
      "0:\tlearn: 0.3626235\ttest: 0.3624668\tbest: 0.3624668 (0)\ttotal: 3.99s\tremaining: 35.9s\n",
      "1:\tlearn: 0.3623695\ttest: 0.3622107\tbest: 0.3622107 (1)\ttotal: 8.01s\tremaining: 32s\n",
      "2:\tlearn: 0.3621402\ttest: 0.3619835\tbest: 0.3619835 (2)\ttotal: 12s\tremaining: 27.9s\n",
      "3:\tlearn: 0.3615724\ttest: 0.3614152\tbest: 0.3614152 (3)\ttotal: 15.9s\tremaining: 23.8s\n",
      "4:\tlearn: 0.3614400\ttest: 0.3612831\tbest: 0.3612831 (4)\ttotal: 20s\tremaining: 20s\n",
      "5:\tlearn: 0.3612426\ttest: 0.3610844\tbest: 0.3610844 (5)\ttotal: 24s\tremaining: 16s\n",
      "6:\tlearn: 0.3611227\ttest: 0.3609655\tbest: 0.3609655 (6)\ttotal: 28.2s\tremaining: 12.1s\n",
      "7:\tlearn: 0.3608456\ttest: 0.3606880\tbest: 0.3606880 (7)\ttotal: 32.1s\tremaining: 8.02s\n",
      "8:\tlearn: 0.3606058\ttest: 0.3604490\tbest: 0.3604490 (8)\ttotal: 36.3s\tremaining: 4.03s\n",
      "9:\tlearn: 0.3604147\ttest: 0.3602599\tbest: 0.3602599 (9)\ttotal: 40.3s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.3602598632\n",
      "bestIteration = 9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier, Pool\n",
    "test_data = catboost_pool = Pool(X_train, \n",
    "                                 y_train)\n",
    "cat_features = [0]\n",
    "train_dataset = Pool(data=X_train,\n",
    "                     label=y_train)\n",
    "\n",
    "eval_dataset = Pool(data=X_valid,\n",
    "                    label=y_valid)\n",
    "\n",
    "test_dataset = Pool(data=x_test,\n",
    "                    label=y_test)\n",
    "model = CatBoostClassifier(iterations=10,\n",
    "                           learning_rate=0.5,\n",
    "                           depth=10,\n",
    "                           loss_function='MultiCrossEntropy',\n",
    "                          l2_leaf_reg = 0.05,\n",
    "                           \n",
    "                          )\n",
    "\n",
    "# Fit model\n",
    "model.fit(train_dataset, eval_set=eval_dataset,verbose_eval=True,use_best_model=True,\n",
    "         early_stopping_rounds=5)\n",
    "\n",
    "model2 = CatBoostClassifier(iterations=10,\n",
    "                           learning_rate=0.15,\n",
    "                           depth=10,\n",
    "                           loss_function='MultiCrossEntropy',\n",
    "                          l2_leaf_reg = 0.02,\n",
    "                          )\n",
    "model2.fit(train_dataset, eval_set=eval_dataset,verbose_eval=True,use_best_model=True,\n",
    "         early_stopping_rounds=5, init_model=model)\n",
    "# Get predicted classes\n",
    "#preds_class = model.predict(test_dataset)\n",
    "# Get predicted probabilities for each class\n",
    "preds_proba = model2.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-standing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#array([0.56572721, 0.60320205, 0.58465289, 0.6038919 , 0.54682868])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "about-microwave",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.69857637, 0.84071803, 0.72615407, 0.73532001, 0.78249558])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_test, preds_proba,  multi_class='ovr', average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-hanging",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-fault",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    " \n",
    "cat = CatBoostClassifier(iterations=7,\n",
    "                           learning_rate=0.7,\n",
    "                           depth=10,\n",
    "                           loss_function='MultiCrossEntropy',\n",
    "                          l2_leaf_reg = 0.05,\n",
    "                          )\n",
    "ovr = OneVsRestClassifier(estimator=cat)\n",
    "ovr.fit(X_train, y_train)\n",
    " \n",
    "preds_proba = ovr.predict_proba(test_x)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-mercury",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(test_y, preds_proba,  multi_class='ovr', average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "completed-chambers",
   "metadata": {},
   "outputs": [],
   "source": [
    "#array([0.56572721, 0.60320205, 0.58465289, 0.6038919 , 0.54682868])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-chosen",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(X_test)\n",
    "predicts = np.array(ovr.predict(X_test))\n",
    "y_test_int = np.argmax(np.array(y_test), axis=1)+1\n",
    "y_prob_int = np.argmax(np.array(predicts), axis=1)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-swedish",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-company",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-mapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_df_interp_bundle_osv_tz.pickle', 'rb') as f:\n",
    "    test_no_target = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-thompson",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_no_target = test_no_target.fillna(0)\n",
    "test_no_target = test_no_target.replace('None', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-oklahoma",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_no_target_dict = encode_features(test_no_target)\n",
    "test_no_target_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-frequency",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-looking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-auckland",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
