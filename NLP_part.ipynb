{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9248e273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33f5a949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting inflect\n",
      "  Downloading inflect-5.3.0-py3-none-any.whl (32 kB)\n",
      "Installing collected packages: inflect\n",
      "Successfully installed inflect-5.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d9fd2103",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bad.pickle', 'rb') as f:\n",
    "    bad_android = pickle.load(f)\n",
    "with open('features.pickle', 'rb') as f:\n",
    "    feature_android = pickle.load(f)\n",
    "with open('features_ios.pickle', 'rb') as f:\n",
    "    features_ios = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "308c2832",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0076546233925290875\n",
      "0.015309246785058175\n",
      "0.02296387017758726\n",
      "0.03061849357011635\n",
      "0.03827311696264544\n",
      "0.04592774035517452\n",
      "0.053582363747703615\n",
      "0.0612369871402327\n",
      "0.06889161053276179\n",
      "0.07654623392529088\n",
      "0.08420085731781997\n",
      "0.09185548071034905\n",
      "0.09951010410287814\n",
      "0.10716472749540723\n",
      "0.11481935088793631\n",
      "0.1224739742804654\n",
      "0.13012859767299448\n",
      "0.13778322106552357\n",
      "0.14543784445805266\n",
      "0.15309246785058175\n",
      "0.16074709124311085\n",
      "0.16840171463563994\n",
      "0.176056338028169\n",
      "0.1837109614206981\n",
      "0.19136558481322719\n",
      "0.19902020820575628\n",
      "0.20667483159828537\n",
      "0.21432945499081446\n",
      "0.22198407838334355\n",
      "0.22963870177587262\n",
      "0.2372933251684017\n",
      "0.2449479485609308\n",
      "0.25260257195345986\n",
      "0.26025719534598896\n",
      "0.26791181873851805\n",
      "0.27556644213104714\n",
      "0.28322106552357623\n",
      "0.2908756889161053\n",
      "0.2985303123086344\n",
      "0.3061849357011635\n",
      "0.3138395590936926\n",
      "0.3214941824862217\n",
      "0.3291488058787508\n",
      "0.3368034292712799\n",
      "0.34445805266380897\n",
      "0.352112676056338\n",
      "0.3597672994488671\n",
      "0.3674219228413962\n",
      "0.3750765462339253\n",
      "0.38273116962645437\n",
      "0.39038579301898346\n",
      "0.39804041641151255\n",
      "0.40569503980404165\n",
      "0.41334966319657074\n",
      "0.42100428658909983\n",
      "0.4286589099816289\n",
      "0.436313533374158\n",
      "0.4439681567666871\n",
      "0.45162278015921614\n",
      "0.45927740355174523\n",
      "0.4669320269442743\n",
      "0.4745866503368034\n",
      "0.4822412737293325\n",
      "0.4898958971218616\n",
      "0.4975505205143907\n",
      "0.5052051439069197\n",
      "0.5128597672994488\n",
      "0.5205143906919779\n",
      "0.528169014084507\n",
      "0.5358236374770361\n",
      "0.5434782608695652\n",
      "0.5511328842620943\n",
      "0.5587875076546234\n",
      "0.5664421310471525\n",
      "0.5740967544396816\n",
      "0.5817513778322106\n",
      "0.5894060012247397\n",
      "0.5970606246172688\n",
      "0.6047152480097979\n",
      "0.612369871402327\n",
      "0.6200244947948561\n",
      "0.6276791181873852\n",
      "0.6353337415799143\n",
      "0.6429883649724434\n",
      "0.6506429883649725\n",
      "0.6582976117575016\n",
      "0.6659522351500307\n",
      "0.6736068585425597\n",
      "0.6812614819350888\n",
      "0.6889161053276179\n",
      "0.6965707287201469\n",
      "0.704225352112676\n",
      "0.7118799755052051\n",
      "0.7195345988977342\n",
      "0.7271892222902633\n",
      "0.7348438456827924\n",
      "0.7424984690753215\n",
      "0.7501530924678506\n",
      "0.7578077158603796\n",
      "0.7654623392529087\n",
      "0.7731169626454378\n",
      "0.7807715860379669\n",
      "0.788426209430496\n",
      "0.7960808328230251\n",
      "0.8037354562155542\n",
      "0.8113900796080833\n",
      "0.8190447030006124\n",
      "0.8266993263931415\n",
      "0.8343539497856706\n",
      "0.8420085731781997\n",
      "0.8496631965707288\n",
      "0.8573178199632578\n",
      "0.8649724433557869\n",
      "0.872627066748316\n",
      "0.8802816901408451\n",
      "0.8879363135333742\n",
      "0.8955909369259032\n",
      "0.9032455603184323\n",
      "0.9109001837109614\n",
      "0.9185548071034905\n",
      "0.9262094304960196\n",
      "0.9338640538885487\n",
      "0.9415186772810777\n",
      "0.9491733006736068\n",
      "0.9568279240661359\n",
      "0.964482547458665\n",
      "0.9721371708511941\n",
      "0.9797917942437232\n",
      "0.9874464176362523\n",
      "0.9951010410287814\n"
     ]
    }
   ],
   "source": [
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import inflect\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def remove_URL(sample):\n",
    "    \"\"\"Remove URLs from a sample string\"\"\"\n",
    "    return re.sub(r\"http\\S+\", \"\", sample)\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "def preprocess(sample):\n",
    "    soup = BeautifulSoup(sample)\n",
    "    sample = soup.get_text()\n",
    "\n",
    "    sample = remove_URL(sample)\n",
    "    # Tokenize\n",
    "    words = nltk.word_tokenize(sample)\n",
    "\n",
    "    # Normalize\n",
    "    words = normalize(words)\n",
    "    words = \" \".join(words)\n",
    "    return words\n",
    "\n",
    "\n",
    "clear_text = []\n",
    "for i, bundle in enumerate(features_ios):\n",
    "    try:\n",
    "        if not pd.isna(features_ios[bundle][\"name\"]) and features_ios[bundle][\"name\"] != \"nan\":\n",
    "            sample = preprocess(features_ios[bundle][\"name\"])\n",
    "            clear_text.append(sample)\n",
    "        else:\n",
    "            clear_text.append(\" \")\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(i/len(features_ios))\n",
    "    except:\n",
    "        print(\"Error\")\n",
    "        clear_text.append(\" \")\n",
    "        continue\n",
    "\n",
    "\n",
    "# clear_text = []\n",
    "# for i, bundle in enumerate(feature_android):\n",
    "#     try:\n",
    "#         if not pd.isna(feature_android[bundle][\"name\"]) and feature_android[bundle][\"name\"] != \"nan\":\n",
    "#             sample = preprocess(feature_android[bundle][\"name\"])\n",
    "#             clear_text.append(sample)\n",
    "#         if i % 100 == 0:\n",
    "#             print(i/len(feature_android))\n",
    "#     except:\n",
    "#         print(\"Error\")\n",
    "#         clear_text.append(\" \")\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9f92c807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from text2vec import SBert\n",
    "\n",
    "sbert_model = SBert('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "vectorize = sbert_model.encode(clear_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fd8d9fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, bundle in enumerate(features_ios):\n",
    "    features_ios[bundle][\"vectorized\"] = np.mean(vectorize[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ea9f84cd",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0076546233925290875\n",
      "0.015309246785058175\n",
      "0.02296387017758726\n",
      "0.03061849357011635\n",
      "0.03827311696264544\n",
      "0.04592774035517452\n",
      "0.053582363747703615\n",
      "0.0612369871402327\n",
      "0.06889161053276179\n",
      "0.07654623392529088\n",
      "0.08420085731781997\n",
      "0.09185548071034905\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1218 is out of bounds for axis 0 with size 1206",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-f014aa6a225a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_ios\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbundle\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"name\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeatures_ios\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbundle\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"name\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"nan\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m             \u001b[0mfeatures_ios\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbundle\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"vectorized\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1218 is out of bounds for axis 0 with size 1206",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-f014aa6a225a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_ios\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mfeatures_ios\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbundle\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"vectorized\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1218 is out of bounds for axis 0 with size 1206"
     ]
    }
   ],
   "source": [
    "for i, bundle in enumerate(features_ios):\n",
    "    try:\n",
    "        if not pd.isna(features_ios[bundle][\"name\"]) and features_ios[bundle][\"name\"] != \"nan\":\n",
    "            features_ios[bundle][\"vectorized\"] = np.mean(vectorize[i]) \n",
    "        if i % 100 == 0:\n",
    "            print(i/len(features_ios))\n",
    "    except:\n",
    "        features_ios[bundle][\"vectorized\"] = np.mean(vectorize[i]) \n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9214184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('feature_android_vectorized.pickle', 'wb') as f:\n",
    "    pickle.dump(feature_android, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "36db0d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('features_ios_vectorized.pickle', 'wb') as f:\n",
    "    pickle.dump(features_ios, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d421e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aa1191",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
